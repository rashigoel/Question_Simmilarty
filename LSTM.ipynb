{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/rashi/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from random import shuffle\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.models import model_from_json\n",
    "from newspaper import Article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lstm_model:\n",
    "    def transform_keywords(self, file_name):\n",
    "        inf_file = open(file_name)\n",
    "        data = list()\n",
    "        for one_news in inf_file.readlines():\n",
    "            single = one_news.strip().split(',')\n",
    "            mapping = list()\n",
    "            for one_keyword in single:\n",
    "                mapping.append(one_hot(one_keyword, 7000)[0])\n",
    "            data.append(mapping)\n",
    "        print(4,data)\n",
    "        return data\n",
    "\n",
    "    def transform_titles(self, file_name):\n",
    "        inf_file = open(file_name)\n",
    "        data = list()\n",
    "        for one_news in inf_file.readlines():\n",
    "            single = nltk.word_tokenize(self.clean_sentence(one_news))\n",
    "            mapping = list()\n",
    "            for one_keyword in single:\n",
    "                mapping.append(one_hot(one_keyword, 7000)[0])\n",
    "            data.append(mapping)\n",
    "        print(5,data)\n",
    "        return data\n",
    "\n",
    "    def clean_sentence(self,s):\n",
    "        print(\"3\",s)\n",
    "        data = \"\"\n",
    "        for item in s:\n",
    "            item.lower().strip()\n",
    "        #c = s.lower().strip()\n",
    "        return re.sub('[^a-z ]', '', \" \".join(s))\n",
    "\n",
    "    '''\n",
    "    :param\n",
    "        type: 0 indicates using the keywords from the content\n",
    "              1 indicates using the titles\n",
    "    '''\n",
    "    def save_model(self, fake_file, real_file, type, model_name, unit_size = 10):\n",
    "        batch_size = 10\n",
    "        if type == 0:\n",
    "            fake_data = self.transform_keywords(fake_file)\n",
    "            real_data = self.transform_keywords(real_file)\n",
    "        else:\n",
    "            fake_data = self.transform_titles(fake_file)\n",
    "            real_data = self.transform_titles(real_file)\n",
    "            batch_size = 32\n",
    "        labels = list()\n",
    "        max_len = 0\n",
    "        for i in fake_data:\n",
    "            labels.append(0)\n",
    "        for i in real_data:\n",
    "            labels.append(1)\n",
    "        data=fake_data\n",
    "        for r in fake_data:\n",
    "            if max_len < len(r):\n",
    "                max_len = len(r)\n",
    "        for r in real_data:\n",
    "            if max_len < len(r):\n",
    "                max_len = len(r)\n",
    "            data.append(r)\n",
    "        for d in data:\n",
    "            cur_len = len(d)\n",
    "            while cur_len < max_len:\n",
    "                d.append(0)\n",
    "                cur_len = cur_len+1\n",
    "\n",
    "        #shuffle the given data\n",
    "        index_shuf = list(range(len(data)))\n",
    "        shuffle(index_shuf)\n",
    "        data_shuffled = list()\n",
    "        label_shuffled = list()\n",
    "        for i in index_shuf:\n",
    "            data_shuffled.append(data[i])\n",
    "            label_shuffled.append(labels[i])\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(7000, 256, dropout=0.2))\n",
    "        model.add(LSTM(16, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        model.fit(data_shuffled, label_shuffled,\n",
    "                  nb_epoch=10,\n",
    "                  batch_size=batch_size,\n",
    "                  shuffle=False)\n",
    "        # serialize model to JSON\n",
    "        model_json = model.to_json()\n",
    "        with open(model_name, \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(\"model.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "\n",
    "    def reload_model(self, file_name):\n",
    "        json_file = open(file_name, 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        loaded_model.load_weights(\"model.h5\")\n",
    "        print(\"Loaded model from disk\")\n",
    "        return loaded_model\n",
    "\n",
    "    #max_len for keywords: , max_len_for_titles:\n",
    "    def format_testcase(self,string, type, max_len):\n",
    "        #titles\n",
    "        single = list()\n",
    "        if type == 0:\n",
    "            single = nltk.word_tokenize(self.clean_sentence(string))\n",
    "        #keywords\n",
    "        else:\n",
    "            single = string\n",
    "        mapping = list()\n",
    "        for one_keyword in single:\n",
    "            mapping.append(one_hot(one_keyword, 7000)[0])\n",
    "        while len(mapping) < max_len:\n",
    "            mapping.append(0)\n",
    "        data = list()\n",
    "        data.append(mapping)\n",
    "        print(data)\n",
    "        return mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bbc', 'crashthe', 'route', 'crashed', 'inexplicable', 'operator', 'killing', 'usual', 'seaplane', 'sydney', 'path']\n",
      "article keywords have been printed\n",
      "4 [[5668], [2456], [5796], [117, 1950], [2855, 3630], [746], [2276], [4959], [4354], [780], [1709], [4673], [4731], [5726, 749], [3436], [2332, 4673], [2638], [6289, 4967], [6582, 6123], [3760], [5857, 6007], [5000], [4673], [951], [6526], [671], [4142, 5419], [107], [5000], [948], [3891], [4967], [6221], [6943, 4846], [3284], [5360], [3279], [4977], [1035], [4673], [5727], [1679], [3388, 376], [5472], [5820], [5636, 819], [3379, 2402, 3581], [3145, 5894], [2281], [5318, 395], [1560], [4411], [749, 4932, 4951], [2685], [2221], [4015], [3359, 547], [1744], [190], [6239], [10, 3880], [1051, 4558], [4015, 4293], [5358], [4372, 4691], [6815], [4160], [1561], [12], [6003, 2644], [641], [3760], [1999], [1806], [6864, 4678], [5433], [4673], [4411], [4902], [5145], [1292], [1806], [4673], [1110], [5416], [3379], [4321], [107], [2124], [3247], [3379], [1806, 4142], [12], [4977], [12], [5416], [2456], [93], [2456], [4673], [5768], [747], [4138], [2456], [1542], [6725, 6007], [2456], [3379], [3170], [749, 1844], [3379], [3237], [3338], [1979, 2644, 2613, 1494], [2519]]\n",
      "4 [[3253], [4687, 19], [749], [4673], [6937], [6208], [3247, 1410], [3891, 4620], [322, 6276], [1292, 5464], [3662], [4754, 5276], [1878], [6282], [4912], [6120], [2067], [4967], [4823], [5091], [6117], [1806], [1764], [4823], [715], [2808], [6141, 6336], [4175], [3995]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rashi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:77: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/home/rashi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:78: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(16, dropout=0.2, recurrent_dropout=0.2)`\n",
      "/home/rashi/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 0.5871 - acc: 0.7986\n",
      "Epoch 2/10\n",
      "144/144 [==============================] - 1s 3ms/step - loss: 0.5117 - acc: 0.7986\n",
      "Epoch 3/10\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 0.4978 - acc: 0.7986\n",
      "Epoch 4/10\n",
      "144/144 [==============================] - 1s 3ms/step - loss: 0.4877 - acc: 0.7986\n",
      "Epoch 5/10\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 0.4799 - acc: 0.7986\n",
      "Epoch 6/10\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 0.4583 - acc: 0.7986\n",
      "Epoch 7/10\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 0.4347 - acc: 0.7986\n",
      "Epoch 8/10\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 0.4017 - acc: 0.7986\n",
      "Epoch 9/10\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 0.3473 - acc: 0.8194\n",
      "Epoch 10/10\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 0.2735 - acc: 0.8681\n",
      "Saved model to disk\n",
      "3 ['bbc', 'crashthe', 'route', 'crashed', 'inexplicable', 'operator', 'killing', 'usual', 'seaplane', 'sydney', 'path']\n",
      "[[1510, 3739, 1508, 367, 3648, 3233, 2553, 3927, 6928, 1891, 4054]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e3e05d4bb699>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_testcase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# change 19 to the corresponding  max_len(19 or 39)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Task on the content keywords\n",
    "    #save_model(\"./fakenews_keywords.csv\",\"./realnews_keywords.csv\",0,\"model_keywords.json\")\n",
    "    #Task on the titles\n",
    "    #save_model(\"./data/titles/fake_news_training.txt\", \"./data/titles/real_news_training.txt\",1,\"model_titles.json\" )\n",
    "    #Redload a save model:\n",
    "    #Extract keywords from the web\n",
    "    article = Article('http://www.bbc.com/news')\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    print(article.keywords)\n",
    "    print(\"article keywords have been printed\")\n",
    "    mod = lstm_model()\n",
    "    model = mod.save_model('data/fake_news_training.txt','data/real_news_training.txt',0,'model_titles.json')\n",
    "\n",
    "    '''parse the input from web front end, say the keywords/titles are raw strings\n",
    "    TODO: 1) Preprocess the string such that only words count\n",
    "          2) Form the corresponding matrix by calling the function like:\n",
    "            (max_len is determined by the training model)\n",
    "            format_testcase(string, type=1 for keywords, 0 for title, max_len = 19 for keywords, =39 for titles):'''\n",
    "\n",
    "\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    X = mod.format_testcase(article.keywords, 0,9)\n",
    "    #result = model.predict(X) # change 19 to the corresponding  max_len(19 or 39)\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from scipy import stats\n",
    "from random import shuffle\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_keywords(file_name):\n",
    "    inf_file = open(file_name)\n",
    "    data = list()\n",
    "    for one_news in inf_file.readlines():\n",
    "        single = one_news.strip().split(',')\n",
    "        mapping = list()\n",
    "        for one_keyword in single:\n",
    "            mapping.append(one_hot(one_keyword, 7000)[0])\n",
    "        data.append(mapping)\n",
    "    #print(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n:param\\n    type: 0 indicates using the keywords from the content\\n          1 indicates using the titles\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_titles(file_name):\n",
    "    inf_file = open(file_name)\n",
    "    data = list()\n",
    "    for one_news in inf_file.readlines():\n",
    "        single = nltk.word_tokenize(clean_sentence(one_news))\n",
    "        print(single)\n",
    "        mapping = list()\n",
    "        for one_keyword in single:\n",
    "            mapping.append(one_hot(one_keyword, 7000)[0])\n",
    "        data.append(mapping)\n",
    "    # print(data)\n",
    "    return data\n",
    "\n",
    "def clean_sentence(s):\n",
    "    c = s.lower().strip()\n",
    "    return re.sub('[^a-z ]', '', c)\n",
    "\n",
    "'''\n",
    ":param\n",
    "    type: 0 indicates using the keywords from the content\n",
    "          1 indicates using the titles\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(fake_file, real_file, type, unit_size = 10):\n",
    "    if type == 0:\n",
    "        fake_data = transform_keywords(fake_file)\n",
    "        real_data = transform_keywords(real_file)\n",
    "    else:\n",
    "        fake_data = transform_titles(fake_file)\n",
    "        real_data = transform_titles(real_file)\n",
    "    labels = list()\n",
    "    max_len = 0\n",
    "    for i in fake_data:\n",
    "        labels.append(0)\n",
    "    for i in real_data:\n",
    "        labels.append(1)\n",
    "    data=fake_data\n",
    "    for r in fake_data:\n",
    "        if max_len < len(r):\n",
    "            max_len = len(r)\n",
    "    for r in real_data:\n",
    "        if max_len < len(r):\n",
    "            max_len = len(r)\n",
    "        data.append(r)\n",
    "    print(max_len)\n",
    "    for d in data:\n",
    "        cur_len = len(d)\n",
    "        while cur_len < max_len:\n",
    "            d.append(0)\n",
    "            cur_len = cur_len+1\n",
    "    print(data)\n",
    "\n",
    "    #shuffle the given data\n",
    "    index_shuf = list(range(len(data)))\n",
    "    shuffle(index_shuf)\n",
    "    data_shuffled = list()\n",
    "    label_shuffled = list()\n",
    "    for i in index_shuf:\n",
    "        data_shuffled.append(data[i])\n",
    "        label_shuffled.append(labels[i])\n",
    "    print(len(label_shuffled))\n",
    "    print(label_shuffled)\n",
    "\n",
    "    # generate cross validation datasets\n",
    "    k = 0\n",
    "    testing_size = len(data_shuffled)/unit_size\n",
    "    training_set_X = list()\n",
    "    training_set_Y = list()\n",
    "    testing_set_X = list()\n",
    "    testing_set_Y = list()\n",
    "\n",
    "    while k < testing_size:\n",
    "        test_X= data_shuffled[k*unit_size:(k+1)*unit_size]\n",
    "        test_Y = label_shuffled[k*unit_size:(k+1)*unit_size]\n",
    "\n",
    "        train_X = data_shuffled[:k * unit_size] + data_shuffled[(k + 1) * unit_size:]\n",
    "        train_Y = label_shuffled[:k * unit_size] + label_shuffled[(k + 1) * unit_size:]\n",
    "\n",
    "        training_set_X.append(train_X)\n",
    "        training_set_Y.append(train_Y)\n",
    "        testing_set_X.append(test_X)\n",
    "        testing_set_Y.append(test_Y)\n",
    "        k = k+1\n",
    "\n",
    "    print(len(training_set_X))\n",
    "    print(training_set_Y)\n",
    "\n",
    "    # testing with the baseline\n",
    "    test_index = 0\n",
    "    while test_index < testing_size:\n",
    "        print('Build model...')\n",
    "        baselineTest = np.float(np.sum(testing_set_Y[test_index])) / unit_size\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(7000, 256, dropout=0.2))\n",
    "        model.add(LSTM(16, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        print('Train...')\n",
    "        model.fit(training_set_X[test_index], training_set_Y[test_index], batch_size=len(testing_set_X[test_index]),\n",
    "                  nb_epoch=10,\n",
    "                  validation_data=(testing_set_X[test_index], testing_set_Y[test_index]), shuffle=False)\n",
    "        score, acc = model.evaluate(testing_set_X[test_index], testing_set_Y[test_index],\n",
    "                                    batch_size=len(testing_set_X[test_index]))\n",
    "        print('Test accuracy:', acc)\n",
    "        print('Baseline: ', str(max(baselineTest,1-baselineTest)))\n",
    "        test_index = test_index +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "[[3918, 100, 3622, 6084, 4793, 5330, 4610, 3582, 4887, 3415, 5993, 2479, 1403, 5425, 1236, 2138, 0, 0, 0], [394, 1443, 100, 4745, 960, 6739, 3161, 5993, 6546, 5082, 2479, 5425, 4749, 3432, 4962, 0, 0, 0, 0], [5056, 4215, 100, 4238, 249, 4365, 2712, 3097, 5743, 5993, 121, 4934, 2479, 1809, 1618, 503, 1233, 4498, 0], [4378, 2479, 1103, 4610, 3582, 6070, 6099, 1342, 4121, 140, 5, 5842, 1924, 3533, 2568, 0, 0, 0, 0], [1765, 4238, 4908, 728, 2543, 3879, 5299, 6846, 6701, 1393, 3195, 2731, 3918, 5866, 5773, 0, 0, 0, 0], [6120, 6986, 4238, 494, 5120, 6480, 846, 4710, 1059, 1819, 3052, 6210, 4875, 649, 2987, 0, 0, 0, 0], [3918, 100, 3622, 6084, 4793, 5330, 4610, 3582, 4887, 3415, 5993, 2479, 1403, 5425, 1236, 2138, 0, 0, 0], [2479, 5239, 5145, 5120, 3747, 6885, 2293, 6312, 4594, 3834, 2500, 3463, 5734, 653, 1095, 0, 0, 0, 0], [5000, 712, 6739, 3713, 2822, 4604, 584, 2227, 5993, 5063, 2479, 5233, 1012, 529, 4962, 838, 0, 0, 0], [2255, 3104, 4078, 5239, 5145, 919, 2206, 3466, 5993, 911, 2069, 463, 1247, 503, 4179, 1095, 5995, 0, 0], [4286, 6767, 4360, 4238, 2395, 2221, 2149, 3097, 2747, 5120, 1444, 722, 1618, 503, 3945, 1097, 5082, 0, 0], [90, 6557, 4298, 2479, 3017, 225, 6739, 665, 1994, 5993, 86, 574, 812, 519, 2556, 4467, 2784, 1688, 0], [6319, 2479, 3715, 3395, 4654, 2845, 4610, 2554, 4325, 5894, 3671, 4248, 5993, 3400, 4518, 2427, 1688, 0, 0], [3726, 5807, 2479, 1524, 27, 3918, 5573, 3607, 1529, 140, 6574, 3844, 6712, 0, 0, 0, 0, 0, 0], [671, 4907, 4238, 6997, 5647, 3607, 5266, 5120, 6294, 3909, 503, 1417, 1253, 3044, 865, 0, 0, 0, 0], [666, 3223, 4238, 3715, 5079, 3588, 4558, 4304, 5120, 6270, 1869, 6731, 5715, 1039, 5522, 1242, 623, 0, 0], [5000, 6938, 4424, 4238, 431, 2553, 4053, 3064, 3097, 4922, 6846, 5120, 5776, 5432, 3195, 1618, 503, 1649, 3811], [130, 3269, 6424, 3529, 111, 925, 698, 619, 6675, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4238, 1137, 1564, 4390, 6564, 4284, 1079, 1393, 5120, 140, 4784, 3195, 2731, 5880, 1093, 330, 0, 0, 0], [3309, 5522, 4238, 6363, 6711, 27, 1753, 2293, 1288, 4861, 1118, 2376, 4754, 3176, 0, 0, 0, 0, 0], [3494, 4238, 3792, 3181, 5120, 3417, 3971, 1813, 3008, 1116, 5146, 0, 0, 0, 0, 0, 0, 0, 0], [337, 1924, 960, 2325, 6515, 2264, 2383, 5993, 456, 2479, 3195, 6739, 649, 0, 0, 0, 0, 0, 0], [902, 5075, 4215, 2986, 4238, 2948, 2005, 5110, 5522, 5120, 2165, 4372, 2731, 15, 5866, 3035, 0, 0, 0], [1675, 4215, 2479, 3713, 2325, 5274, 6881, 5993, 6294, 4494, 5899, 6739, 4466, 3945, 346, 3740, 0, 0, 0], [1649, 3279, 4238, 616, 2573, 5582, 3582, 5082, 722, 6892, 649, 1381, 2124, 0, 0, 0, 0, 0, 0], [3309, 5512, 4238, 1564, 4390, 1980, 6140, 1109, 4539, 1346, 5082, 2376, 3195, 649, 3481, 0, 0, 0, 0], [6319, 2479, 3715, 3395, 4654, 2845, 4610, 2554, 4325, 5894, 3671, 4248, 5993, 3400, 4518, 2427, 1688, 0, 0], [5809, 3400, 2479, 2845, 4354, 239, 4325, 5894, 3671, 5993, 6319, 3195, 2515, 1816, 0, 0, 0, 0, 0], [3607, 2001, 6010, 3998, 4238, 3700, 6477, 1796, 4610, 6385, 6883, 1443, 6913, 2731, 6884, 1145, 0, 0, 0], [1590, 3769, 3054, 832, 5824, 5157, 1340, 1059, 6090, 4219, 3052, 6217, 5859, 2084, 747, 0, 0, 0, 0], [6986, 4238, 960, 4886, 4237, 846, 1994, 5120, 3949, 5082, 1419, 4865, 0, 0, 0, 0, 0, 0, 0], [576, 1443, 5708, 297, 2106, 3607, 2978, 6415, 5522, 507, 2981, 0, 0, 0, 0, 0, 0, 0, 0], [358, 5423, 6672, 4105, 5599, 2035, 4176, 5982, 5587, 2766, 4210, 4877, 5120, 2307, 2932, 0, 0, 0, 0], [807, 4519, 5058, 4867, 163, 2325, 262, 5458, 436, 4307, 3886, 696, 3341, 4294, 5522, 0, 0, 0, 0], [2001, 960, 3969, 2325, 4539, 5458, 1135, 6997, 2011, 5772, 3195, 5260, 503, 5522, 649, 3481, 623, 0, 0], [6801, 1834, 2344, 1928, 3582, 2767, 6067, 3962, 4395, 4372, 2164, 0, 0, 0, 0, 0, 0, 0, 0], [5768, 4914, 1096, 2382, 3624, 1003, 880, 5522, 3972, 46, 2712, 4462, 5314, 3195, 5773, 5734, 356, 0, 0], [4537, 4078, 4065, 6787, 3646, 4874, 4603, 4932, 1247, 693, 4372, 5467, 2449, 0, 0, 0, 0, 0, 0], [6073, 4238, 474, 2770, 4769, 5120, 5772, 6968, 4874, 3195, 2634, 1143, 5773, 0, 0, 0, 0, 0, 0], [1762, 6120, 3269, 3299, 504, 5837, 6026, 6787, 3154, 3089, 4637, 3052, 3991, 3533, 1501, 0, 0, 0, 0], [2113, 4350, 4378, 2479, 634, 626, 4835, 2014, 6211, 5583, 5943, 5875, 2560, 311, 1393, 3496, 1981, 0, 0], [4050, 3969, 3296, 2500, 3935, 4610, 2589, 6742, 6638, 6372, 5770, 5660, 4186, 5522, 0, 0, 0, 0, 0], [2215, 1096, 4395, 1230, 6760, 880, 6647, 1942, 2712, 4462, 6767, 5522, 1420, 5734, 121, 5773, 0, 0, 0], [4350, 3177, 2479, 897, 311, 3913, 2198, 2014, 6943, 336, 2165, 1721, 2670, 0, 0, 0, 0, 0, 0], [3856, 5647, 4238, 5476, 2636, 2060, 1928, 5120, 6885, 2767, 3503, 2749, 5901, 4410, 2398, 0, 0, 0, 0], [3582, 4238, 2305, 1564, 2500, 4365, 4539, 5120, 1346, 1382, 4932, 1740, 3481, 2987, 0, 0, 0, 0, 0], [5358, 2001, 3867, 1823, 4278, 2969, 2206, 5770, 6568, 4093, 5208, 1052, 0, 0, 0, 0, 0, 0, 0], [5358, 1396, 2001, 2761, 3867, 4686, 4278, 2969, 2206, 6594, 5770, 1494, 5680, 6568, 5208, 1052, 0, 0, 0], [5358, 5770, 1052, 482, 3867, 4172, 4278, 1854, 6350, 2206, 2001, 6568, 3619, 5208, 2969, 0, 0, 0, 0], [5358, 2001, 3867, 5239, 4278, 2969, 2206, 2448, 5770, 2419, 6568, 5441, 5208, 1052, 0, 0, 0, 0, 0], [293, 1353, 2325, 3792, 2395, 5210, 5926, 5522, 5458, 6895, 5180, 6690, 6574, 696, 0, 0, 0, 0, 0], [5738, 3777, 5556, 3140, 4640, 4024, 5081, 5523, 4251, 2292, 4093, 2003, 317, 3814, 5734, 4432, 0, 0, 0], [3372, 6703, 6822, 6678, 1793, 3734, 1321, 6881, 2781, 1764, 4700, 6221, 546, 1074, 5522, 0, 0, 0, 0], [4237, 6986, 5522, 880, 3529, 4640, 3415, 673, 5035, 5337, 4365, 4462, 3195, 4467, 5734, 5773, 0, 0, 0], [2001, 6986, 2023, 5429, 72, 4372, 4633, 6204, 4729, 6497, 6731, 15, 503, 0, 0, 0, 0, 0, 0], [2001, 5793, 3969, 2500, 2325, 5522, 2441, 6315, 5458, 6968, 3195, 3726, 649, 3481, 5082, 0, 0, 0, 0], [3954, 4238, 4190, 4525, 1928, 2767, 4710, 5120, 446, 5082, 6450, 3195, 6760, 4196, 649, 5859, 0, 0, 0], [2874, 3551, 2590, 2282, 5234, 3161, 1290, 3132, 5053, 5691, 4474, 4250, 5656, 0, 0, 0, 0, 0, 0], [4238, 4190, 4525, 1928, 2767, 3055, 572, 5120, 446, 6450, 5386, 3195, 6760, 4196, 649, 5859, 0, 0, 0], [4024, 217, 6997, 126, 3683, 2219, 6446, 6294, 888, 3052, 3991, 1779, 1715, 5522, 0, 0, 0, 0, 0], [4883, 3195, 4102, 87, 2107, 3898, 4710, 3646, 4686, 6320, 2974, 4594, 2434, 1217, 0, 0, 0, 0, 0], [5793, 3867, 5448, 4278, 4473, 2339, 5895, 61, 2023, 4429, 1831, 3594, 5522, 0, 0, 0, 0, 0, 0], [960, 6488, 2479, 4238, 626, 2952, 4473, 5457, 31, 3607, 5120, 3851, 1393, 4582, 6892, 3481, 0, 0, 0], [6426, 3223, 3582, 6015, 2380, 6249, 6312, 851, 3564, 3170, 927, 4467, 431, 623, 0, 0, 0, 0, 0], [6883, 3223, 3867, 3582, 5522, 216, 338, 1326, 1203, 6690, 5712, 1618, 5796, 4467, 665, 623, 0, 0, 0], [4877, 2395, 3481, 4065, 3526, 729, 3891, 5583, 3052, 6315, 3543, 5244, 790, 5521, 0, 0, 0, 0, 0], [6426, 4467, 3223, 1321, 3582, 5337, 6284, 4603, 927, 623, 1181, 216, 0, 0, 0, 0, 0, 0, 0], [4238, 4779, 2500, 4157, 4610, 1367, 942, 6064, 5120, 4179, 3982, 5754, 503, 3481, 0, 0, 0, 0, 0], [350, 2987, 6910, 4238, 2231, 3834, 1929, 5573, 5788, 1407, 3867, 4919, 1409, 5862, 3195, 1327, 4307, 5289, 0], [4549, 217, 804, 2173, 2035, 6312, 4610, 6439, 100, 2332, 4372, 1475, 3152, 5289, 0, 0, 0, 0, 0], [1608, 2382, 1300, 2282, 3415, 3331, 1095, 4865, 2788, 6952, 6916, 1823, 1403, 2010, 5773, 0, 0, 0, 0], [1823, 1443, 1403, 1092, 1936, 1773, 5020, 2862, 3064, 2788, 6916, 5729, 1813, 6952, 6688, 4238, 0, 0, 0], [1823, 5128, 6309, 6, 4489, 5867, 2785, 6608, 3817, 2788, 6952, 3295, 1236, 1012, 5734, 5346, 0, 0, 0], [1249, 2078, 4238, 1936, 4365, 574, 5208, 1608, 4594, 1403, 5729, 3415, 5655, 2862, 6712, 0, 0, 0, 0], [2172, 4268, 5470, 2383, 4072, 2881, 6415, 3616, 3582, 4933, 1224, 86, 667, 4265, 4696, 829, 4251, 2544, 0], [3955, 1155, 6005, 4570, 5931, 3681, 6562, 6549, 328, 55, 871, 5432, 5205, 6680, 2574, 2011, 0, 0, 0], [1610, 5106, 3919, 3999, 3544, 4865, 6782, 2656, 887, 300, 5522, 1561, 5307, 0, 0, 0, 0, 0, 0], [3430, 2282, 729, 1012, 6198, 4749, 5573, 3208, 172, 3896, 2511, 3195, 34, 3503, 0, 0, 0, 0, 0], [3920, 813, 2986, 6083, 1978, 4595, 6056, 5855, 696, 5978, 2667, 790, 5697, 0, 0, 0, 0, 0, 0], [6954, 6952, 734, 569, 876, 1113, 2244, 4946, 1665, 1088, 3906, 2788, 6921, 431, 1823, 6710, 2541, 0, 0], [6767, 4238, 6496, 5120, 3879, 4044, 5994, 2731, 4865, 2987, 5862, 6258, 3247, 2495, 5392, 714, 0, 0, 0], [6916, 5053, 3919, 3890, 4116, 4973, 2788, 649, 3432, 6952, 4010, 4208, 571, 1823, 2084, 777, 3993, 0, 0], [2974, 3842, 1109, 3942, 4238, 5862, 2383, 3195, 657, 2731, 2836, 0, 0, 0, 0, 0, 0, 0, 0], [1655, 6657, 5978, 6083, 5641, 4117, 665, 22, 6056, 1206, 3920, 3718, 2667, 144, 5688, 0, 0, 0, 0], [5534, 1201, 4908, 4498, 5647, 1313, 6222, 165, 3582, 3186, 1494, 6217, 2617, 270, 6700, 5734, 4840, 368, 0], [5871, 4270, 6630, 6129, 5330, 6430, 323, 4514, 2533, 5559, 1494, 1012, 1190, 644, 0, 0, 0, 0, 0], [358, 1544, 6198, 4238, 1287, 6715, 2386, 313, 5866, 336, 4242, 209, 2902, 3195, 3844, 1190, 0, 0, 0], [3059, 1558, 4238, 6983, 6911, 4093, 1310, 6734, 165, 4979, 2920, 6563, 0, 0, 0, 0, 0, 0, 0], [6892, 5784, 4470, 4837, 4420, 2862, 4065, 4133, 3700, 6721, 1312, 3999, 6754, 3646, 451, 0, 0, 0, 0], [1083, 5243, 4365, 4914, 6457, 3170, 3343, 2190, 3118, 1791, 2436, 1727, 3362, 2987, 0, 0, 0, 0, 0]]\n",
      "90\n",
      "[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0]\n",
      "9\n",
      "[[1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]\n",
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rashi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:71: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/home/rashi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:72: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(16, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rashi/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.6707 - acc: 0.7125 - val_loss: 0.6308 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.6155 - acc: 0.7500 - val_loss: 0.5761 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.5659 - acc: 0.7500 - val_loss: 0.5342 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.5238 - acc: 0.7500 - val_loss: 0.5152 - val_acc: 0.8000\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.4928 - acc: 0.7500 - val_loss: 0.5113 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.4514 - acc: 0.7500 - val_loss: 0.5050 - val_acc: 0.8000\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.3841 - acc: 0.7875 - val_loss: 0.4919 - val_acc: 0.8000\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 0.2628 - acc: 0.8875 - val_loss: 0.4792 - val_acc: 0.8000\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.1742 - acc: 0.9625 - val_loss: 0.5002 - val_acc: 0.8000\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.1043 - acc: 1.0000 - val_loss: 0.5391 - val_acc: 0.8000\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "Test accuracy: 0.800000011921\n",
      "Baseline:  0.8\n",
      "Build model...\n",
      "Train...\n",
      "Train on 80 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.6521 - acc: 0.7500 - val_loss: 0.6248 - val_acc: 0.7000\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.5752 - acc: 0.7625 - val_loss: 0.5891 - val_acc: 0.7000\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.5118 - acc: 0.7625 - val_loss: 0.5794 - val_acc: 0.7000\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 0.4565 - acc: 0.7625 - val_loss: 0.5751 - val_acc: 0.7000\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.3782 - acc: 0.7875 - val_loss: 0.5351 - val_acc: 0.7000\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.2626 - acc: 0.9125 - val_loss: 0.4325 - val_acc: 0.7000\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.1547 - acc: 0.9750 - val_loss: 0.3293 - val_acc: 0.8000\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.0980 - acc: 1.0000 - val_loss: 0.2677 - val_acc: 0.9000\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.0630 - acc: 1.0000 - val_loss: 0.2414 - val_acc: 0.9000\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.0440 - acc: 1.0000 - val_loss: 0.2352 - val_acc: 0.8000\n",
      "10/10 [==============================] - 0s 645us/step\n",
      "Test accuracy: 0.800000011921\n",
      "Baseline:  0.7\n",
      "Build model...\n",
      "Train...\n",
      "Train on 80 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.6638 - acc: 0.7500 - val_loss: 0.6136 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.6034 - acc: 0.7500 - val_loss: 0.5528 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.5538 - acc: 0.7500 - val_loss: 0.5040 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.5092 - acc: 0.7500 - val_loss: 0.4734 - val_acc: 0.8000\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.4693 - acc: 0.7500 - val_loss: 0.4410 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.3881 - acc: 0.7875 - val_loss: 0.3776 - val_acc: 0.8000\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.2830 - acc: 0.8625 - val_loss: 0.2787 - val_acc: 0.8000\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.1816 - acc: 0.9625 - val_loss: 0.2035 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.1099 - acc: 1.0000 - val_loss: 0.1852 - val_acc: 0.9000\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0767 - acc: 1.0000 - val_loss: 0.1941 - val_acc: 0.9000\n",
      "10/10 [==============================] - 0s 437us/step\n",
      "Test accuracy: 0.899999976158\n",
      "Baseline:  0.8\n",
      "Build model...\n",
      "Train...\n",
      "Train on 80 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.6707 - acc: 0.6875 - val_loss: 0.6254 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.6044 - acc: 0.7500 - val_loss: 0.5735 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.5452 - acc: 0.7500 - val_loss: 0.5513 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.5055 - acc: 0.7500 - val_loss: 0.5588 - val_acc: 0.8000\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.4580 - acc: 0.7500 - val_loss: 0.5703 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.3733 - acc: 0.7750 - val_loss: 0.5642 - val_acc: 0.8000\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.2543 - acc: 0.8875 - val_loss: 0.5073 - val_acc: 0.8000\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.1689 - acc: 0.9750 - val_loss: 0.3839 - val_acc: 0.8000\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.1008 - acc: 1.0000 - val_loss: 0.2720 - val_acc: 0.8000\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0670 - acc: 1.0000 - val_loss: 0.2421 - val_acc: 0.9000\n",
      "10/10 [==============================] - 0s 546us/step\n",
      "Test accuracy: 0.899999976158\n",
      "Baseline:  0.8\n",
      "Build model...\n",
      "Train...\n",
      "Train on 80 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.6700 - acc: 0.6125 - val_loss: 0.6410 - val_acc: 0.7000\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.5925 - acc: 0.7625 - val_loss: 0.6066 - val_acc: 0.7000\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.5339 - acc: 0.7625 - val_loss: 0.6003 - val_acc: 0.7000\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.4837 - acc: 0.7625 - val_loss: 0.6168 - val_acc: 0.7000\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.4108 - acc: 0.7750 - val_loss: 0.6220 - val_acc: 0.7000\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.3060 - acc: 0.8750 - val_loss: 0.5898 - val_acc: 0.7000\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.2015 - acc: 0.9250 - val_loss: 0.5336 - val_acc: 0.7000\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.1189 - acc: 1.0000 - val_loss: 0.5022 - val_acc: 0.7000\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0859 - acc: 1.0000 - val_loss: 0.5505 - val_acc: 0.7000\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0610 - acc: 1.0000 - val_loss: 0.6507 - val_acc: 0.6000\n",
      "10/10 [==============================] - 0s 351us/step\n",
      "Test accuracy: 0.600000023842\n",
      "Baseline:  0.7\n",
      "Build model...\n",
      "Train...\n",
      "Train on 80 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 2s 19ms/step - loss: 0.6755 - acc: 0.6625 - val_loss: 0.6026 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.6097 - acc: 0.7500 - val_loss: 0.5112 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.5590 - acc: 0.7500 - val_loss: 0.4382 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.5194 - acc: 0.7500 - val_loss: 0.3966 - val_acc: 0.8000\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.4754 - acc: 0.7500 - val_loss: 0.3720 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.4012 - acc: 0.7750 - val_loss: 0.3475 - val_acc: 0.8000\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2777 - acc: 0.8750 - val_loss: 0.3262 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1698 - acc: 0.9875 - val_loss: 0.3396 - val_acc: 0.8000\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1044 - acc: 1.0000 - val_loss: 0.4268 - val_acc: 0.8000\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0688 - acc: 1.0000 - val_loss: 0.5242 - val_acc: 0.8000\n",
      "10/10 [==============================] - 0s 442us/step\n",
      "Test accuracy: 0.800000011921\n",
      "Baseline:  0.8\n",
      "Build model...\n",
      "Train...\n",
      "Train on 80 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.6763 - acc: 0.6500 - val_loss: 0.5932 - val_acc: 0.9000\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.6104 - acc: 0.7375 - val_loss: 0.4873 - val_acc: 0.9000\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5533 - acc: 0.7375 - val_loss: 0.3902 - val_acc: 0.9000\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.5139 - acc: 0.7375 - val_loss: 0.3193 - val_acc: 0.9000\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.4473 - acc: 0.7500 - val_loss: 0.2663 - val_acc: 0.9000\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.3374 - acc: 0.8250 - val_loss: 0.2041 - val_acc: 0.9000\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2174 - acc: 0.9625 - val_loss: 0.1326 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.1387 - acc: 1.0000 - val_loss: 0.0814 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0864 - acc: 1.0000 - val_loss: 0.0531 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0601 - acc: 1.0000 - val_loss: 0.0366 - val_acc: 1.0000\n",
      "10/10 [==============================] - 0s 727us/step\n",
      "Test accuracy: 1.0\n",
      "Baseline:  0.9\n",
      "Build model...\n",
      "Train...\n",
      "Train on 80 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.6654 - acc: 0.6875 - val_loss: 0.6286 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.5890 - acc: 0.7500 - val_loss: 0.5781 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.5284 - acc: 0.7500 - val_loss: 0.5441 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.4877 - acc: 0.7500 - val_loss: 0.5201 - val_acc: 0.8000\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 0.4418 - acc: 0.7625 - val_loss: 0.4842 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 0.3570 - acc: 0.8250 - val_loss: 0.4132 - val_acc: 0.8000\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 0.2171 - acc: 0.9250 - val_loss: 0.2870 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.1260 - acc: 1.0000 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0879 - acc: 1.0000 - val_loss: 0.0915 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.0526 - acc: 1.0000 - val_loss: 0.0605 - val_acc: 1.0000\n",
      "10/10 [==============================] - 0s 693us/step\n",
      "Test accuracy: 1.0\n",
      "Baseline:  0.8\n",
      "Build model...\n",
      "Train...\n",
      "Train on 80 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 2s 24ms/step - loss: 0.6575 - acc: 0.6875 - val_loss: 0.6961 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.5619 - acc: 0.7875 - val_loss: 0.7287 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.4888 - acc: 0.7875 - val_loss: 0.8011 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.4510 - acc: 0.7875 - val_loss: 0.8611 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.4065 - acc: 0.7875 - val_loss: 0.8461 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.3249 - acc: 0.8250 - val_loss: 0.7625 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.2022 - acc: 0.9250 - val_loss: 0.6512 - val_acc: 0.6000\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.1093 - acc: 1.0000 - val_loss: 0.5848 - val_acc: 0.6000\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0744 - acc: 1.0000 - val_loss: 0.6061 - val_acc: 0.7000\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0472 - acc: 1.0000 - val_loss: 0.6396 - val_acc: 0.7000\n",
      "10/10 [==============================] - 0s 428us/step\n",
      "Test accuracy: 0.699999988079\n",
      "Baseline:  0.5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Task on the content keywords\n",
    "    make_prediction(\"./fakenews_keywords.csv\",\"./realnews_keywords.csv\",0)\n",
    "    #Task on the titles\n",
    "    #make_prediction(\"./data/titles/fake_news_training.txt\", \"./data/titles/real_news_training.txt\",1 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "#import feature_matrix\n",
    "#import tensorflow\n",
    "import string\n",
    "from itertools import chain\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import RandomTree as rt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.preprocessing.text import one_hot\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "%run /home/rashi/Desktop/Fakenews/utils.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_STOPWORDS = stopwords.words('english') + list(string.punctuation)\n",
    "def signatures(ambiguous_word, pos=None,\n",
    "               hyperhypo=True, adapted=False,\n",
    "               remove_stopwords=True, to_lemmatize=True):\n",
    "    # Ensure that the POS is supported.\n",
    "    pos = pos if pos in ['a', 'r', 's', 'n', 'v', None] else None\n",
    "    # Holds the synset->signature dictionary.\n",
    "    synsets_signatures = {}\n",
    "    for ss in wn.synsets(ambiguous_word, pos=pos):\n",
    "        signature = []\n",
    "        # Adds the definition, example sentences and lemma_names.\n",
    "        signature += word_tokenize(ss.definition())\n",
    "        signature += chain(*[word_tokenize(eg) for eg in ss.examples()])\n",
    "        signature += ss.lemma_names()\n",
    "        # Optional: includes lemma_names of hyper-/hyponyms.\n",
    "        if hyperhypo:\n",
    "            hyperhypo = set(ss.hyponyms() + ss.hypernyms() + ss.instance_hyponyms() + ss.instance_hypernyms())\n",
    "            signature += set(chain(*[i.lemma_names() for i in hyperhypo]))\n",
    "        # Optional: Includes signatures from related senses as in Adapted Lesk.\n",
    "        if adapted:\n",
    "            # Includes lemma_names from holonyms, meronyms and similar_tos\n",
    "            related_senses = set(ss.member_holonyms() + ss.part_holonyms() + ss.substance_holonyms() +\n",
    "                                 ss.member_meronyms() + ss.part_meronyms() + ss.substance_meronyms() +\n",
    "                                 ss.similar_tos())\n",
    "            signature += set(chain(*[i.lemma_names() for i in related_senses]))\n",
    "        # Optional: removes stopwords.\n",
    "        if remove_stopwords:\n",
    "            signature = [i for i in signature if i not in EN_STOPWORDS]\n",
    "        # Lemmatized context is preferred over stemmed context.\n",
    "        if to_lemmatize:\n",
    "            signature = [lemmatize(i) for i in signature]\n",
    "        synsets_signatures[ss] = signature\n",
    "    return synsets_signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_overlaps_greedy(context, synsets_signatures):\n",
    "    \"\"\"\n",
    "    Calculate overlaps between the context sentence and the synset_signature\n",
    "    and returns the synset with the highest overlap.\n",
    "    Note: Greedy algorithm only keeps the best sense,\n",
    "    see https://en.wikipedia.org/wiki/Greedy_algorithm\n",
    "    Only used by original_lesk(). Keeping greedy algorithm for documentary sake,\n",
    "    because original_lesks is greedy.\n",
    "    \"\"\"\n",
    "    max_overlaps = 0; lesk_sense = None\n",
    "    for ss in synsets_signatures:\n",
    "        overlaps = set(synsets_signatures[ss]).intersection(context)\n",
    "        if len(overlaps) > max_overlaps:\n",
    "            lesk_sense = ss\n",
    "            max_overlaps = len(overlaps)\n",
    "    return lesk_sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_overlaps(context, synsets_signatures, \\\n",
    "                     nbest=False, keepscore=False, normalizescore=False):\n",
    "    \"\"\"\n",
    "    Calculates overlaps between the context sentence and the synset_signture\n",
    "    and returns a ranked list of synsets from highest overlap to lowest.\n",
    "    \"\"\"\n",
    "    overlaplen_synsets = [] # a tuple of (len(overlap), synset).\n",
    "    for ss in synsets_signatures:\n",
    "        overlaps = set(synsets_signatures[ss]).intersection(context)\n",
    "        overlaplen_synsets.append((len(overlaps), ss))\n",
    "\n",
    "    # Rank synsets from highest to lowest overlap.\n",
    "    ranked_synsets = sorted(overlaplen_synsets, reverse=True)\n",
    "\n",
    "    # Normalize scores such that it's between 0 to 1.\n",
    "    if normalizescore:\n",
    "        total = float(sum(i[0] for i in ranked_synsets))\n",
    "        ranked_synsets = [(i/total,j) for i,j in ranked_synsets]\n",
    "\n",
    "    if not keepscore: # Returns a list of ranked synsets without scores\n",
    "        ranked_synsets = [i[1] for i in sorted(overlaplen_synsets, \\\n",
    "                                               reverse=True)]\n",
    "\n",
    "    if nbest: # Returns a ranked list of synsets.\n",
    "        return ranked_synsets\n",
    "    else: # Returns only the best sense.\n",
    "        return ranked_synsets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_signature(ambiguous_word, pos=None, lemma=True, stem=False, \\\n",
    "                     hyperhypo=True, stop=True):\n",
    "    \"\"\"\n",
    "    Returns a synsets_signatures dictionary that includes signature words of a\n",
    "    sense from its:\n",
    "    (i)   definition\n",
    "    (ii)  example sentences\n",
    "    (iii) hypernyms and hyponyms\n",
    "    \"\"\"\n",
    "    synsets_signatures = {}\n",
    "    for ss in wn.synsets(ambiguous_word):\n",
    "        try: # If POS is specified.\n",
    "            if pos and str(ss.pos()) != pos:\n",
    "                continue\n",
    "        except:\n",
    "            if pos and str(ss.pos) != pos:\n",
    "                continue\n",
    "        signature = []\n",
    "        # Includes definition.\n",
    "        ss_definition = synset_properties(ss, 'definition')\n",
    "        signature+=ss_definition\n",
    "        # Includes examples\n",
    "        ss_examples = synset_properties(ss, 'examples')\n",
    "        signature+=list(chain(*[i.split() for i in ss_examples]))\n",
    "        # Includes lemma_names.\n",
    "        ss_lemma_names = synset_properties(ss, 'lemma_names')\n",
    "        signature+= ss_lemma_names\n",
    "\n",
    "        # Optional: includes lemma_names of hypernyms and hyponyms.\n",
    "        if hyperhypo == True:\n",
    "            ss_hyponyms = synset_properties(ss, 'hyponyms')\n",
    "            ss_hypernyms = synset_properties(ss, 'hypernyms')\n",
    "            ss_hypohypernyms = ss_hypernyms+ss_hyponyms\n",
    "            signature+= list(chain(*[i.lemma_names() for i in ss_hypohypernyms]))\n",
    "\n",
    "        # Optional: removes stopwords.\n",
    "        if stop == True:\n",
    "            signature = [i for i in signature if i not in EN_STOPWORDS]\n",
    "        # Lemmatized context is preferred over stemmed context.\n",
    "        if lemma == True:\n",
    "            signature = [lemmatize(i) for i in signature]\n",
    "        # Matching exact words may cause sparsity, so optional matching for stems.\n",
    "        if stem == True:\n",
    "            signature = [porter.stem(i) for i in signature]\n",
    "        synsets_signatures[ss] = signature\n",
    "\n",
    "    return synsets_signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adapted_lesk(context_sentence, ambiguous_word, \\\n",
    "                pos=None, lemma=True, stem=True, hyperhypo=True, \\\n",
    "                stop=True, context_is_lemmatized=False, \\\n",
    "                nbest=False, keepscore=False, normalizescore=False):\n",
    "    \"\"\"\n",
    "    This function is the implementation of the Adapted Lesk algorithm,\n",
    "    described in Banerjee and Pederson (2002). It makes use of the lexical\n",
    "    items from semantically related senses within the wordnet\n",
    "    hierarchies and to generate more lexical items for each sense.\n",
    "    see www.d.umn.edu/~tpederse/Pubs/cicling2002-b.pdfâ€Ž\n",
    "    \"\"\"\n",
    "    # Ensure that ambiguous word is a lemma.\n",
    "    ambiguous_word = lemmatize(ambiguous_word)\n",
    "    # If ambiguous word not in WordNet return None\n",
    "    if not wn.synsets(ambiguous_word):\n",
    "        return None\n",
    "    # Get the signatures for each synset.\n",
    "    ss_sign = simple_signature(ambiguous_word, pos, lemma, stem, hyperhypo)\n",
    "    signature = []\n",
    "    tt = \"\"\n",
    "    for ss in ss_sign:\n",
    "        # Includes holonyms.\n",
    "        ss_mem_holonyms = synset_properties(ss, 'member_holonyms')\n",
    "        ss_part_holonyms = synset_properties(ss, 'part_holonyms')\n",
    "        ss_sub_holonyms = synset_properties(ss, 'substance_holonyms')\n",
    "        # Includes meronyms.\n",
    "        ss_mem_meronyms = synset_properties(ss, 'member_meronyms')\n",
    "        ss_part_meronyms = synset_properties(ss, 'part_meronyms')\n",
    "        ss_sub_meronyms = synset_properties(ss, 'substance_meronyms')\n",
    "        # Includes similar_tos\n",
    "        ss_simto = synset_properties(ss, 'similar_tos')\n",
    "\n",
    "        related_senses = list(set(ss_mem_holonyms+ss_part_holonyms+\n",
    "                                  ss_sub_holonyms+ss_mem_meronyms+\n",
    "                                  ss_part_meronyms+ss_sub_meronyms+ ss_simto))\n",
    "\n",
    "        signature = list([j for j in chain(*[synset_properties(i, 'lemma_names')\n",
    "                                             for i in related_senses])\n",
    "                          if j not in EN_STOPWORDS])\n",
    "        tt = ss\n",
    "\n",
    "    # Lemmatized context is preferred over stemmed context\n",
    "    if lemma == True:\n",
    "        signature = [lemmatize(i) for i in signature]\n",
    "    # Matching exact words causes sparsity, so optional matching for stems.\n",
    "    if stem == True:\n",
    "        signature = [porter.stem(i) for i in signature]\n",
    "    # Adds the extended signature to the simple signatures.\n",
    "    ss_sign[tt]+=signature\n",
    "\n",
    "    # Disambiguate the sense in context.\n",
    "    if context_is_lemmatized:\n",
    "        context_sentence = context_sentence.split()\n",
    "    else:\n",
    "        context_sentence = lemmatize_sentence(context_sentence)\n",
    "    best_sense = compare_overlaps(context_sentence, ss_sign, \\\n",
    "                                    nbest=nbest, keepscore=keepscore, \\\n",
    "                                    normalizescore=normalizescore)\n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to extract adjectives from the contents. Return a list of strings\n",
    "#Need nltk library: import nltk\n",
    "def extract_adjective(sentences):\n",
    "    adj_sentences = list()\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        adj_tags = nltk.pos_tag(words)\n",
    "        one_adj_sentence = \"\"\n",
    "        for index, tag in enumerate(adj_tags, start = 0):\n",
    "            one_tag = tag[1]\n",
    "            if one_tag in ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']:\n",
    "                sense = adapted_lesk(sentence, words[index], one_tag)\n",
    "                #print('\\n\\n')\n",
    "                #print(sense)\n",
    "                if not sense:\n",
    "                    one_adj_sentence += words[index]\n",
    "                if sense:\n",
    "                    one_adj_sentence += sense.lemmas()[0].name()\n",
    "                    #print(sense.lemmas()[0].name())\n",
    "                one_adj_sentence += \" \"\n",
    "        adj_sentences.append(one_adj_sentence)\n",
    "        #print(one_adj_sentence)\n",
    "    return adj_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(s):\n",
    "    c = s.lower().strip()\n",
    "    return re.sub('[^a-z ]', '', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_words(sentences):\n",
    "    encoded = list()\n",
    "    for sentence in sentences:\n",
    "        words = list()\n",
    "        mapping = list()\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            mapping.append(one_hot(word,10000)[0])\n",
    "        encoded.append(mapping);\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_with_max_len(encoded,length):\n",
    "    for e in encoded:\n",
    "        for count in range(len(e), length):\n",
    "            e.append(0)\n",
    "    return encoded;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_label(fake_size, real_size):\n",
    "    label = list()\n",
    "    for counter in range(0,fake_size):\n",
    "        label.append(0)\n",
    "    for counter in range(0,real_size):\n",
    "        label.append(1)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-1727dd2f3779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mpredict_news\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mcount1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_adjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_news\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#print(words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-e382af5e79b3>\u001b[0m in \u001b[0;36mextract_adjective\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mone_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mone_tag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'JJ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'JJR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'JJS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RBR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RBS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0msense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapted_lesk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0;31m#print('\\n\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;31m#print(sense)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-206-cf14bdfe234c>\u001b[0m in \u001b[0;36madapted_lesk\u001b[0;34m(context_sentence, ambiguous_word, pos, lemma, stem, hyperhypo, stop, context_is_lemmatized, nbest, keepscore, normalizescore)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Adds the extended signature to the simple signatures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mss_sign\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Disambiguate the sense in context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "train_news = list()\n",
    "fake_size = 0\n",
    "real_size = 0\n",
    "with open('./data/titles/fake_news_training.txt') as train1:\n",
    "    with open('./data/titles/real_news_training.txt') as train2:\n",
    "        for line in train1:\n",
    "            train_news.append(clean_sentence(line))\n",
    "            fake_size = fake_size+1\n",
    "        for line in train2:\n",
    "            train_news.append(clean_sentence(line))\n",
    "            real_size = real_size+1\n",
    "predict_news = list()\n",
    "count = 0\n",
    "count1 = 0\n",
    "with open('./data/real2.txt') as predict1:\n",
    "    with open('./data/fake2.txt') as predict2:\n",
    "        for line in predict1:\n",
    "            predict_news.append(clean_sentence(line))\n",
    "            count = count+1\n",
    "        for line in predict2:\n",
    "            predict_news.append(clean_sentence(line))\n",
    "            count1 = count1+1\n",
    "words = encode_words(extract_adjective(train_news))    \n",
    "#print(words)\n",
    "\n",
    "#print(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\")\n",
    "#print(count)\n",
    "\n",
    "#print(count1)\n",
    "labels = set_label(fake_size, real_size)\n",
    "#converted = convert_matrix(words)\n",
    "#print(len(converted))\n",
    "appended = append_with_max_len(words,len(max(words, key=len)))\n",
    "#print(appended)\n",
    "predict_words = encode_words(predict_news)\n",
    "classif = OneVsRestClassifier(estimator=SVC(random_state=0))\n",
    "#classif = tree.DecisionTreeClassifier()\n",
    "#classif = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "predict_words = encode_words(extract_adjective(predict_news))\n",
    "item = classif.fit(appended, labels)\n",
    "#print(item)\n",
    "item.predict(append_with_max_len(predict_words,len(max(words, key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
